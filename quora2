from bs4 import BeautifulSoup
import urllib.request
from nltk.tokenize import RegexpTokenizer
import hunspell
import pandas as pd
import os
from difflib import SequenceMatcher
import re


hobj = hunspell.HunSpell('/usr/share/hunspell/en_US.dic', '/usr/share/hunspell/en_US.aff')
tokenizer = RegexpTokenizer(r'\w+')


def similar(a, b):
    '''
    finds how similar the url is to the other urls, and returns if it is acceptable
    :param a: the url
    :param url: the list of urls
    :return: bool or if it is acceptable or not vary the number at the bottom s well
    '''
    similarity = 0
    similarity += SequenceMatcher(None, a[20:], b[-1][20:]).ratio()
    print(b)
    print(similarity, a)
    return similarity > .6


def make_dictionary(answer_counter, question_counter, url, columns, mispelled, q_text, length):
    '''creates a dictionary that can be used in making a table

    :param answer_counter: answer number
    :param question_counter: question number
    :param url: url
    :param columns: column header
    :param mispelled: list of mispelled words
    :param q_text:
    :param length:
    :return:
    '''
    text_info = {}
    text_info[columns[0]] = url
    text_info[columns[1]] = question_counter
    text_info[columns[2]] = q_text
    text_info[columns[3]] = answer_counter
    text_info[columns[4]] = mispelled
    text_info[columns[5]] = len(mispelled)
    text_info[columns[6]] = length
    return text_info


def check_spelling(text):
    '''creates a list of misspelled words in text
    :param text: the text
    :return: list of errors
    '''
    incorrect = []
    length = 0
    words = tokenizer.tokenize(text)
    for element in words:
        length += 1
        if not hobj.spell(element):
            incorrect.append(element)
    return incorrect, length


def find_answers(soup, answer_counter, question_counter, url, columns):
    """
    Goes through the soup object to find all divs that contain the answer class (Answer AnswerBase) then searches within
    those tags for the <p> tags which contain the actual text for the answer and then creates a new txt file which
    contains that text
    :param soup: the soup object from BeautifulSoup and allows for the parsing of the website
    :return: returns the count of how many text files have been made so more files can continue to be made
    """
    dictonaries = []
    divs = soup.find_all("div", class_="Answer AnswerBase")  # finds all the div tags with the answer class
    qdiv = soup.find_all("h1")
    for q in qdiv:
        question_text = q.find("span", class_="ui_qtext_rendered_qtext")
        question_counter += 1
    for d in divs:
        answers = d.find_all("p")  # within the span tags finds all the paragraph tags so answers can be kept together
        answer_counter += 1
        all_mispelled = set()
        length = 0
        with open(str(answer_counter) + '_Experiences in life_' + str(question_counter) + ".txt", "w+") as f:
            for a in answers:
                f.write(a.text)  # writes each answer in a separate text file
                f.write("\n")
                mispelled, line_length = check_spelling(a.text)
                length += line_length
                all_mispelled.update(set(mispelled))
        dictonary = make_dictionary(answer_counter, question_counter, url, columns, list(all_mispelled), question_text.text, length)
        dictonaries.append(dictonary)
    return answer_counter, question_counter, dictonaries


def soup_given_url(given_url):
    """
    Takes in a url then using the BeautifulSoup library, creates a soup object which then can be parsed
    :param given_url: the url which you wish to go to and create the soup object from
    :return: returns the soup object back to main so it can be worked with and parsed
    """
    url = given_url
    content = urllib.request.urlopen(url)
    soup = BeautifulSoup(content, "html.parser")
    return soup


def get_url(soup, urls):
    '''
    looks through the related questions, and finds a good one
    :param soup: soup of the webpage
    :param urls: list of urls, used to make sure we dont find the same one twice
    :return: the new url
    '''
    questions = soup.find_all("li", {"class": "related_question"})
    for question in questions:
        url = question.find('a')['href']
        print(url)
        url = ('https://www.quora.com'+ url)
        searchObj = re.search('you', url, re.M | re.I)
        if url not in urls and similar(url, urls) and searchObj:
            break
    return url


def get_dictionaries(first_url, questions, columns):
    '''
    for the number of questions you want, it finds the questions, and creates a list of dictionaries with the answers
    :param first_url: the url of the first question
    :param questions: number of questions that are wanted
    :param columns: colums for making the dictionaries
    :return: the list of dictionaries
    '''
    list_of_dictionaries = []
    answer_count = 0
    question_counter = 0
    url = first_url
    urls = []
    for i in range(0, questions):
        urls.append(url)
        print('urls left: '+str(questions-i))
        soup = soup_given_url(url)
        answer_count, question_counter, dictionary = find_answers(soup, answer_count, question_counter, url, columns)
        list_of_dictionaries += dictionary
        url = get_url(soup, urls)
    return list_of_dictionaries


def main(topic):
    '''
    main function calls all the other functions and creates the text files with answers
    :param topic: the topic in quora
    '''
    os.makedirs("/home/downey/PycharmProjects/quora/texts")
    # then we change the directory that python looks at to the new place.
    os.chdir("/home/downey/PycharmProjects/quora/texts")

    columns = ['URL', 'Q_id', 'Q_text', 'answer_id', 'list_of_mispelled', 'length_of_mispelled', 'length_of_text']

    dictionaries = get_dictionaries('https://www.quora.com/What-is-the-biggest-scam-you%E2%80%99ve-ever-seen', 5, columns)

    table = pd.DataFrame.from_records(dictionaries, columns=columns)
    table.to_csv(path_or_buf='experiences_table.csv')

main(str('Experiences-in-Life'))
