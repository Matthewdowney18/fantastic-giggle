from bs4 import BeautifulSoup
import urllib.request
import nltk
from nltk.tokenize import RegexpTokenizer
import hunspell
import pandas as pd
import os
from difflib import SequenceMatcher
import re


hobj = hunspell.HunSpell('/usr/share/hunspell/en_US.dic', '/usr/share/hunspell/en_US.aff')
tokenizer = RegexpTokenizer(r'\w+')


def check_page(soup):
    topics_list = []
    topics = soup.find_all("span", {"class": "TopicNameSpan TopicName"})
    topics += (soup.find_all("span", {"class": "TopicName TopicNameSpan"}))
    print(topics)
    for topic in topics:
        topics_list.append(topic.text)
    print(topics_list)
    length = len(soup.find_all("div", class_="Answer AnswerBase"))
    return 'Experiences in Life' in topics_list and length >= 10


def similar_nouns(a, b):
    used_nouns = set()
    new_nouns = set()
    for url in b:
        words = tokenizer.tokenize(url)
        tagged_words = nltk.pos_tag(words)
        for tagged_word in tagged_words[4:]:
            if tagged_word[1] == 'NN':
                used_nouns.add(tagged_word[0])
    words = tokenizer.tokenize(a)
    tagged_words = nltk.pos_tag(words)
    for tagged_word in tagged_words[4:]:
        if tagged_word[1] == 'NN':
            new_nouns.add(tagged_word[0])
    return bool(used_nouns & new_nouns)


def similar(a, b):
    '''
    finds how similar the url is to the other urls, and returns if it is acceptable
    :param a: the url
    :param url: the list of urls
    :return: bool or if it is acceptable or not vary the number at the bottom s well
    '''
    similarity = 0
    similarity += SequenceMatcher(None, a[20:], b[-1][20:]).ratio()

    return similarity > .65


def make_dictionary(answer_counter, question_counter, url, columns, mispelled, q_text, length):
    '''creates a dictionary that can be used in making a table

    :param answer_counter: answer number
    :param question_counter: question number
    :param url: url
    :param columns: column header
    :param mispelled: list of mispelled words
    :param q_text:
    :param length:
    :return:
    '''
    text_info = {}
    text_info[columns[0]] = url
    text_info[columns[1]] = question_counter
    text_info[columns[2]] = q_text
    text_info[columns[3]] = answer_counter
    text_info[columns[4]] = mispelled
    text_info[columns[5]] = len(mispelled)
    text_info[columns[6]] = length
    return text_info


def check_spelling(text):
    '''creates a list of misspelled words in text
    :param text: the text
    :return: list of errors
    '''
    incorrect = []
    length = 0
    words = tokenizer.tokenize(text)
    for element in words:
        length += 1
        if not hobj.spell(element):
            incorrect.append(element)
    return incorrect, length


def find_answers(soup, answer_counter, question_counter, url, columns):
    """
    Goes through the soup object to find all divs that contain the answer class (Answer AnswerBase) then searches within
    those tags for the <p> tags which contain the actual text for the answer and then creates a new txt file which
    contains that text
    :param soup: the soup object from BeautifulSoup and allows for the parsing of the website
    :return: returns the count of how many text files have been made so more files can continue to be made
    """
    bad_count = 0
    dictonaries = []
    divs = soup.find_all("div", class_="Answer AnswerBase")  # finds all the div tags with the answer class
    qdiv = soup.find_all("h1")
    for q in qdiv:
        question_text = q.find("span", class_="ui_qtext_rendered_qtext")
        question_counter += 1
    for d in divs:
        answers = d.find_all("p")  # within the div tags finds all the paragraph tags so answers can be kept together
        answer_counter += 1
        all_mispelled = set()
        length = 0
        with open(str(answer_counter) + '_Experiences in life_' + str(question_counter) + ".txt", "w+") as f:
            for a in answers:
                bad_count = filter_url_or_answer(a.text)
                if bad_count != 0:
                    break
                f.write(a.text)  # writes each answer in a separate text file
                f.write("\n")
                mispelled, line_length = check_spelling(a.text)
                length += line_length
                all_mispelled.update(set(mispelled))
        dictonary = make_dictionary(answer_counter, question_counter, url, columns, list(all_mispelled), question_text.text, length)
        dictonaries.append(dictonary)
    return answer_counter, question_counter, dictonaries


def soup_given_url(given_url):
    """
    Takes in a url then using the BeautifulSoup library, creates a soup object which then can be parsed
    :param given_url: the url which you wish to go to and create the soup object from
    :return: returns the soup object back to main so it can be worked with and parsed
    """
    url = given_url
    content = urllib.request.urlopen(url)
    soup = BeautifulSoup(content, "html.parser")
    return soup


def get_url(soup, urls):
    '''
    looks through the related questions, and finds a good one
    :param soup: soup of the webpage
    :param urls: list of urls, used to make sure we dont find the same one twice
    :return: the new url
    '''
<<<<<<< HEAD
    i = 0
=======
    bad_count = 0
>>>>>>> 51b2181ea5c65637747069f2831cdfcf8d8f0e3d
    questions = soup.find_all("li", {"class": "related_question"})
    for question in questions:
        url = question.find('a')['href']
        url = ('https://www.quora.com'+ url)
        searchObj = re.search('you', url, re.M | re.I)
        i += 1
        if url not in urls and searchObj and not similar_nouns(url, urls):
            print('url # '+ str(i))
            break
        bad_count = filter_url_or_answer(url)
        if bad_count == 0:
            break
    return url

def filter_url_or_answer(string):
    """
    a huge list of dirty/inappropriate/bad words are included in this function so that any string passed into this
    function will be compared to the list. If the string is found to be containing any of the words the counter for
    bad words will go up and be returned. Essentially a function that is just used for filtering
    :param string: any string to be compared against the list, in the current case, urls and answers
    :return: the number of bad words found in the string
    """
    bad_count = 0
    arrBad = [
        '2g1c', '2 girls 1 cup', 'acrotomophilia', 'anal', 'anilingus', 'anus', 'arsehole', 'ass', 'asshole', 'assmunch'
        , 'auto erotic', 'autoerotic', 'babeland', 'baby batter', 'ball gag', 'ball gravy', 'ball kicking',
        'ball licking', 'ball sack', 'ball sucking', 'bangbros', 'bareback', 'barely legal', 'barenaked', 'bastardo',
        'bastinado', 'bbw', 'bdsm', 'beaver cleaver', 'beaver lips', 'bestiality', 'bi curious', 'big black',
        'big breasts', 'big knockers', 'big tits', 'bimbos', 'birdlock', 'bitch', 'black cock', 'blonde action',
        'blonde on blonde action', 'blow j', 'blow your l', 'blue waffle', 'blumpkin', 'bollocks', 'bondage', 'boner',
        'boob', 'boobs', 'booty call', 'brown showers', 'brunette action', 'bukkake', 'bulldyke', 'bullet vibe',
        'bung hole', 'bunghole', 'busty', 'butt', 'buttcheeks', 'butthole', 'camel toe', 'camgirl', 'camslut',
        'camwhore', 'carpet muncher', 'carpetmuncher', 'chocolate rosebuds', 'circlejerk', 'cleveland steamer',
        'clit', 'clitoris', 'clover clamps', 'clusterfuck', 'cock', 'cocks', 'coprolagnia', 'coprophilia', 'cornhole',
        'cum', 'cumming', 'cunnilingus', 'cunt', 'darkie', 'date rape', 'daterape', 'deep throat', 'deepthroat',
        'dick', 'dildo', 'dirty pillows', 'dirty sanchez', 'dog style', 'doggie style', 'doggiestyle', 'doggy style',
        'doggystyle', 'dolcett', 'domination', 'dominatrix', 'dommes', 'donkey punch', 'double dong',
        'double penetration', 'dp action', 'eat my ass', 'ecchi', 'ejaculation', 'erotic', 'erotism', 'escort',
        'ethical slut', 'eunuch', 'faggot', 'fecal', 'felch', 'fellatio', 'feltch', 'female squirting', 'femdom',
        'figging', 'fingering', 'fisting', 'foot fetish', 'footjob', 'frotting', 'fuck', 'fucking', 'fuck buttons',
        'fudge packer', 'fudgepacker', 'futanari', 'g-spot', 'gang bang', 'gay sex', 'genitals', 'giant cock',
        'girl on', 'girl on top', 'girls gone wild', 'goatcx', 'goatse', 'gokkun', 'golden shower', 'goo girl',
        'goodpoop', 'goregasm', 'grope', 'group sex', 'guro', 'hand job', 'handjob', 'hard core', 'hardcore', 'hentai',
        'homoerotic', 'honkey', 'hooker', 'hot chick', 'how to kill', 'how to murder', 'huge fat', 'humping', 'incest',
        'intercourse', 'jack off', 'jail bait', 'jailbait', 'jerk off', 'jigaboo', 'jiggaboo', 'jiggerboo', 'jizz',
        'juggs', 'kike', 'kinbaku', 'kinkster', 'kinky', 'knobbing', 'leather restraint', 'leather straight jacket',
        'lemon party', 'lolita', 'lovemaking', 'make me come', 'male squirting', 'masturbate', 'menage a trois',
        'milf', 'missionary position', 'motherfucker', 'mound of venus', 'mr hands', 'muff diver', 'muffdiving',
        'nambla', 'nawashi', 'negro', 'neonazi', 'nig nog', 'nigga', 'nigger', 'nimphomania', 'nipple', 'nipples',
        'nsfw images', 'nude', 'nudity', 'nympho', 'nymphomania', 'octopussy', 'omorashi', 'one cup two girls',
        'one guy one jar', 'orgasm', 'orgy', 'paedophile', 'panties', 'panty', 'pedobear', 'pedophile', 'pegging',
        'penis', 'phone sex', 'piece of shit', 'piss pig', 'pissing', 'pisspig', 'playboy', 'pleasure chest',
        'pole smoker', 'ponyplay', 'poof', 'poop chute', 'poopchute', 'porn', 'porno', 'pornography',
        'prince albert piercing', 'pthc', 'pubes', 'pussy', 'queaf', 'raghead', 'raging boner', 'rape', 'raping',
        'rapist', 'rectum', 'reverse cowgirl', 'rimjob', 'rimming', 'rosy palm', 'rosy palm and her 5 sisters',
        'rusty trombone', 's&m', 'sadism', 'scat', 'schlong', 'scissoring', 'semen', 'sex', 'sexo', 'sexy',
        'shaved beaver', 'shaved pussy', 'shemale', 'shibari', 'shit', 'shota', 'shrimping', 'slanteye', 'slut', 'smut',
        'snatch', 'snowballing', 'sodomize', 'sodomy', 'spic', 'spooge', 'spread legs', 'strap on', 'strapon',
        'strappado', 'strip club', 'style doggy', 'suck', 'sucks', 'suicide girls', 'sultry women', 'swastika',
        'swinger', 'tainted love', 'taste my', 'tea bagging', 'threesome', 'throating', 'tied up', 'tight white', 'tit',
        'tits', 'titties', 'titty', 'tongue in a', 'topless', 'tosser', 'towelhead', 'tranny', 'tribadism', 'tub girl',
        'tubgirl', 'tushy', 'twat', 'twink', 'twinkie', 'two girls one cup', 'undressing', 'upskirt', 'urethra play',
        'urophilia', 'vagina', 'venus mound', 'vibrator', 'violet blue', 'violet wand', 'vorarephilia', 'voyeur',
        'vulva', 'wank', 'wet dream', 'wetback', 'white power', 'women rapping', 'wrapping men', 'wrinkled starfish',
        'xx', 'xxx', 'yaoi', 'yellow showers', 'yiffy', 'zoophilia']
    for word in arrBad:
        if word in string:
            bad_count += 1
    return bad_count



def get_dictionaries(first_url, questions, columns):
    '''
    for the number of questions you want, it finds the questions, and creates a list of dictionaries with the answers
    :param first_url: the url of the first question
    :param questions: number of questions that are wanted
    :param columns: colums for making the dictionaries
    :return: the list of dictionaries
    '''
    list_of_dictionaries = []
    answer_count = 0
    question_counter = 0
    url = first_url
    urls = []
    i = 0
    while i < questions:
        urls.append(url)
        print('urls left: '+str(questions-i))
        soup = soup_given_url(url)
        if check_page(soup):
            answer_count, question_counter, dictionary = find_answers(soup, answer_count, question_counter, url, columns)
            list_of_dictionaries += dictionary
            i += 1
            print('its good')
        url = get_url(soup, urls)
        print(url)
        print('---------')
    return list_of_dictionaries


def main(topic):
    '''
    main function calls all the other functions and creates the text files with answers
    :param topic: the topic in quora
    '''
    os.makedirs("/home/downey/PycharmProjects/quora/texts")
    # then we change the directory that python looks at to the new place.
    os.chdir("/home/downey/PycharmProjects/quora/texts")

    columns = ['URL', 'Q_id', 'Q_text', 'answer_id', 'list_of_mispelled', 'length_of_mispelled', 'length_of_text']

    dictionaries = get_dictionaries('https://www.quora.com/What-is-your-biggest-regret-in-life-1', 3, columns)

    table = pd.DataFrame.from_records(dictionaries, columns=columns)
    table.to_csv(path_or_buf='experiences_table.csv')

main(str('Experiences-in-Life'))
